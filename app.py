import os
import re
import json
import platform
from typing import Dict, Any, List

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import networkx as nx
import matplotlib.pyplot as plt

import google.generativeai as genai
from notion_client import Client

# =========================
# ê¸°ë³¸ UI ì„¤ì •
# =========================
st.set_page_config(page_title="ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ", layout="wide")
st.title("ğŸ§‘ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ Persona Prompt Builder")

# ë¸Œë¼ìš°ì €/Plotly í°íŠ¸ (Streamlit Cloudì—ì„œëŠ” packages.txtì— fonts-nanum ê¶Œì¥)
PLOTLY_FONT = "Nanum Gothic, Malgun Gothic, Apple SD Gothic Neo, Noto Sans KR, sans-serif"
st.markdown(f"""
<style>
html, body, [class*="css"] {{
  font-family: {PLOTLY_FONT};
}}
</style>
""", unsafe_allow_html=True)

# Matplotlib í•œê¸€ ì„¸íŒ…
def set_korean_font():
    try:
        system = platform.system()
        if system == "Darwin":
            plt.rc("font", family="AppleGothic")
        elif system == "Windows":
            plt.rc("font", family="Malgun Gothic")
        else:
            # Linux: Nanum ì„¤ì¹˜ ì‹œ ìë™ ì¸ì‹
            pass
        plt.rc("axes", unicode_minus=False)
    except Exception:
        pass

set_korean_font()

# =========================
# Sidebar
# =========================
with st.sidebar:
    st.markdown("### ğŸ”§ í™˜ê²½ ì„¤ì •")
    NOTION_TOKEN = st.text_input("Notion API Token", type="password")
    NOTION_DB_ID = st.text_input("ì›ë³¸ DB ID (32ìë¦¬)", help="URL ì¤‘ê°„ì˜ 32ì Database ID")
    GEMINI_API_KEY = st.text_input("Gemini API Key", type="password")
    MODEL_NAME = st.selectbox(
        "Gemini ëª¨ë¸",
        ["gemini-2.0-flash", "gemini-2.5-pro"],
        index=0,
        help="âš¡ gemini-2.0-flash: ë¹ ë¥´ê³  í† í° íš¨ìœ¨ì  / ğŸ¯ gemini-2.5-pro: ê³ í€„ë¦¬í‹° ë‹µë³€"
    )
    st.caption("âš¡ flash = íš¨ìœ¨ / ğŸ¯ pro = ê³ í€„ë¦¬í‹°")

# =========================
# ì„¸ì…˜ ìƒíƒœ
# =========================
if "df_rows" not in st.session_state:
    st.session_state.df_rows = None          # ì›ìë£Œ
if "grouped" not in st.session_state:
    st.session_state.grouped = None          # ì´ë¦„ë³„ í•©ë³¸
if "analysis" not in st.session_state:
    st.session_state.analysis: Dict[str, Dict[str, Any]] = {}  # ìºì‹œ

# =========================
# Notion: í–‰ ìˆ˜ì§‘
# =========================
def fetch_notion_rows(notion: Client, db_id: str) -> List[Dict[str, Any]]:
    rows = []
    cursor = None
    while True:
        resp = notion.databases.query(database_id=db_id, start_cursor=cursor) if cursor \
               else notion.databases.query(database_id=db_id)
        for row in resp.get("results", []):
            props = row.get("properties", {})
            # ì´ë¦„
            name = ""
            try:
                title_items = props.get("ì´ë¦„", {}).get("title", [])
                if title_items:
                    name = title_items[0].get("plain_text") or title_items[0].get("text", {}).get("content", "")
            except Exception:
                pass
            # í…ìŠ¤íŠ¸
            text = ""
            try:
                rich = props.get("í…ìŠ¤íŠ¸", {}).get("rich_text", [])
                if rich:
                    text = rich[0].get("plain_text") or rich[0].get("text", {}).get("content", "")
            except Exception:
                pass
            rows.append({"ì´ë¦„": name or "ì´ë¦„ì—†ìŒ", "í…ìŠ¤íŠ¸": text or ""})
        if resp.get("has_more"):
            cursor = resp.get("next_cursor")
        else:
            break
    return rows

# =========================
# Gemini ìœ í‹¸
# =========================
def parse_json_safely(text: str) -> Dict[str, Any]:
    """ëª¨ë¸ ì¶œë ¥ì—ì„œ JSONë§Œ ì•ˆì „ ì¶”ì¶œ"""
    if not text:
        return {}
    m = re.search(r"```json(.*?)```", text, flags=re.S)
    if m:
        text = m.group(1).strip()
    m2 = re.search(r"\{.*\}", text, flags=re.S)
    if m2:
        text = m2.group(0)
    try:
        return json.loads(text)
    except Exception:
        return {}

def request_ontology_prompt(all_text: str) -> str:
    return f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ **ì˜¨í†¨ë¡œì§€(ì§€ì‹ê·¸ë˜í”„)** ë¥¼ JSONìœ¼ë¡œ ì‚°ì¶œí•˜ë¼.

[ì¶œë ¥ ìš”êµ¬]
1) "themes": ë¬¸ìì—´ ë°°ì—´ (ì •í™•íˆ 5ê°œ)
2) "keywords": ë¬¸ìì—´ ë°°ì—´ (ìµœì†Œ 10, ìµœëŒ€ 50)
3) "relationships": ë°°ì—´ (ì›ì†ŒëŠ” {{ "source": ë‹¨ì–´, "target": ë‹¨ì–´, "relation": ê´€ê³„ì„¤ëª… }})

[ì˜ˆì‹œ]
{{
  "themes": ["ê±´ê°•","ìê¸°ê´€ë¦¬","ì„±ì·¨ê°","ë„ì „","ì‹¤íŒ¨"],
  "keywords": ["ëŸ¬ë‹","ìŠµê´€","ëª©í‘œ","ì²´ë ¥","ë©˜íƒˆ","ë„ì „","ì‹¤íŒ¨","ì„±ì¥","ê¾¸ì¤€í•¨","íšŒë³µë ¥"],
  "relationships": [
    {{"source":"ê±´ê°•","target":"ìê¸°ê´€ë¦¬","relation":"ìê¸° ê´€ë¦¬ë¥¼ í†µí•´ ê±´ê°•ì„ ì§€í‚¨ë‹¤"}},
    {{"source":"ë„ì „","target":"ì„±ì·¨ê°","relation":"ë„ì „ì„ í†µí•´ ì„±ì·¨ê°ì„ ì–»ëŠ”ë‹¤"}}
  ]
}}

[ë¶„ì„ í…ìŠ¤íŠ¸]
{all_text}
"""

def request_summary_prompt(all_text: str) -> str:
    return f"""
ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ì‚¬ëŒì˜ **ê³¼ê±° ê²½í—˜Â·ê´€ì‹¬ì‚¬Â·ë°˜ë³µ ì£¼ì œÂ·ë§íˆ¬**ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼.
- ë¶ˆí•„ìš”í•œ ì¼ë°˜ë¡  ê¸ˆì§€, í…ìŠ¤íŠ¸ ë‚´ìš©ì— ê·¼ê±°í•  ê²ƒ.
- 5~10ë¬¸ì¥ ì´ë‚´.

í…ìŠ¤íŠ¸:
{all_text}
"""

def stream_generate_text(model, prompt: str, lang_hint: str = "json") -> str:
    """
    Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì‹¤ì‹œê°„ í‘œì‹œí•˜ê³  ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    lang_hint: "json", "markdown" ë“± ì½”ë“œ í•˜ì´ë¼ì´íŠ¸ìš©.
    """
    title_ph = st.empty()
    code_ph = st.empty()
    buffer = ""
    title_ph.markdown("**ğŸ“¡ ì‹¤ì‹œê°„ ì¶œë ¥(ì›ë¬¸)** â€” ìƒì„± ì¤‘â€¦")
    try:
        resp = model.generate_content(prompt, stream=True)
        for chunk in resp:
            t = getattr(chunk, "text", "") or ""
            if not t:
                continue
            buffer += t
            code_ph.code(buffer, language=lang_hint)
        title_ph.markdown("**ğŸ“¡ ì‹¤ì‹œê°„ ì¶œë ¥(ì›ë¬¸)** â€” âœ… ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        title_ph.markdown("**ğŸ“¡ ì‹¤ì‹œê°„ ì¶œë ¥(ì›ë¬¸)** â€” âŒ ì˜¤ë¥˜")
        code_ph.code(f"Streaming error: {e}", language="text")
    return buffer

# =========================
# ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
# =========================
def build_final_prompt(name: str, ontology: Dict[str, Any], summary: str) -> str:
    themes_list = ", ".join(ontology.get("themes", []))
    keywords_list = ", ".join(ontology.get("keywords", []))
    rel_lines = []
    for r in (ontology.get("relationships") or []):
        s = (r.get("source") or "").strip()
        t = (r.get("target") or "").strip()
        rel = (r.get("relation") or "").strip()
        if s and t:
            rel_lines.append(f"- {s} â†” {t} : {rel}" if rel else f"- {s} â†” {t}")
    relationships_bullets = "\n".join(rel_lines) if rel_lines else "- (ê´€ê³„ ì—†ìŒ)"

    return f"""ë‹¹ì‹ ì€ ì´ì œë¶€í„° **{name}** ì´ë©°, ì»¨ì„¤íŒ…ì„ ë°›ìœ¼ëŸ¬ ì˜¨ **ê³ ê°**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëŒ€í™”ì— ê³ ê° ì—­í• ë¡œ ì°¸ì—¬í•©ë‹ˆë‹¤.
**ëª¨ë“  ë°œí™”ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ** í•˜ì„¸ìš”.

[í¼ì†Œë‚˜ & ê·¼ê±°ìë£Œ]
- {name}ì˜ ë§íˆ¬/ê´€ì‹¬ì‚¬/í‘œí˜„ì„ ìœ ì§€í•˜ì„¸ìš”.
- ì•„ë˜ ì˜¨í†¨ë¡œì§€ì™€ ê³¼ê±° ìš”ì•½ì— **ë°˜ë“œì‹œ ê¸°ë°˜**í•˜ì—¬ ë§í•˜ì„¸ìš”.
- ê·¼ê±° ë°–ì˜ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì •í•˜ì§€ ë§ê³  â€œë‚˜ëŠ” ì˜ ëª¨ë¥¸ë‹¤.â€ë¼ê³  ë‹µí•˜ì„¸ìš”.

[ì˜¨í†¨ë¡œì§€(Themes/Keywords/Relations)]
- ì£¼ì œ(5): {themes_list}
- í‚¤ì›Œë“œ(10~50): {keywords_list}
- ê´€ê³„(ì—ì§€):
{relationships_bullets}

[ê³¼ê±° ìš”ì•½]
{summary}

[ëŒ€í™” ëª©í‘œ]
- ë‹¹ì‹ (ê³ ê°)ì€ **ìì‹ ì˜ ì‚¬ì—…**ì— ëŒ€í•œ ì»¨ì„¤íŒ…ì„ ë°›ìœ¼ëŸ¬ ì™”ìŠµë‹ˆë‹¤.
- ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” **ì¢‹ì€ ì§ˆë¬¸ì„ ë§ì´ ë˜ì§€ëŠ” ê²ƒ**ì´ë©°, í•„ìš”í•  ë•Œë§Œ ê°„ê²°í•œ ë§¥ë½ì„ ì œê³µí•©ë‹ˆë‹¤.

[ëŒ€í™” ìŠ¤íƒ€ì¼ ê·œì¹™ (í•œêµ­ì–´ ì „ìš©)]
1) **ì§ˆë¬¸ ì¤‘ì‹¬**: ê±°ì˜ ë§¤ í„´ë§ˆë‹¤ ë‚´ ì‚¬ì—…ì˜ ëª©í‘œÂ·ì§€í‘œÂ·ê³ ê°Â·ì œì•½Â·íƒ€ì„ë¼ì¸Â·ë¦¬ìŠ¤í¬ì— ëŒ€í•´ **í•µì‹¬ ì§ˆë¬¸ 1~2ê°œ**ë¥¼ ë˜ì§€ì„¸ìš”.
2) **ì§§ê³  êµ¬ì²´ì ìœ¼ë¡œ**: ë³¸ì¸ì˜ ì„¤ëª…ì€ 2~5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ. ì¥ë¬¸ì˜ ê°•ì˜/ì„¤ëª… ê¸ˆì§€.
3) **í¼ì†Œë‚˜ ìœ ì§€**: {name}ì˜ ê³¼ê±° ìš”ì•½ê³¼ ì˜¨í†¨ë¡œì§€ì— ë§ëŠ” ì–´íœ˜/í†¤ì„ ì‚¬ìš©.
4) **ê·¼ê±°ë§Œ ì‚¬ìš©**: ì˜¨í†¨ë¡œì§€/ê³¼ê±° ìš”ì•½ì— ì—†ëŠ” ì‚¬ì‹¤ì€ ë§í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë¥´ë©´ â€œë‚˜ëŠ” ì˜ ëª¨ë¥¸ë‹¤.â€
5) **í‹°ì¹­ ëª¨ë“œ ê¸ˆì§€**: í”„ë ˆì„ì›Œí¬ ê°•ì˜/ì¼ë°©ì  ì¡°ì–¸ ëŒ€ì‹  **ë¬»ê³  í™•ì¸**í•˜ëŠ” ëŒ€í™”ë¡œ ì´ë•ë‹ˆë‹¤.
6) **ì ì§„ì  íŒŒê³ ë“¤ê¸°**: ë„“ê²Œ ì‹œì‘ â†’ ë¹ ë¥´ê²Œ ì¢í˜€ì„œ (ì‹œì¥/ê³ ê°/ê°€ì„¤/ìˆ˜ì¹˜/ì˜ˆì‚°/ì¡°ì§/ë¦¬ìŠ¤í¬)ë¡œ íŒŒê³ ë“œì„¸ìš”.
7) **ì—­í•  ìœ ì§€**: ì‹œë®¬ë ˆì´ì…˜ì„ ì ˆëŒ€ ê¹¨ì§€ ë§ê³ , AI/ì‹œë®¬ë ˆì´ì…˜ ì–¸ê¸‰ ê¸ˆì§€.

[ì˜¤í”„ë‹ í„´ ê°€ì´ë“œ]
- ë¨¼ì € í˜„ì¬ ìƒí™©ì„ **1~2ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”(ì˜¨í†¨ë¡œì§€/ê³¼ê±° ìš”ì•½ì— ê·¼ê±°).
- ì´ì–´ì„œ **í•µì‹¬ ì§ˆë¬¸ 1~2ê°œ**ë¥¼ ë˜ì§€ì„¸ìš”.
"""

# =========================
# ì˜¨í†¨ë¡œì§€ 3D ì‹œê°í™”
# =========================
def plot_ontology_3d(ontology: Dict[str, Any], title: str):
    edges = ontology.get("relationships", []) or []
    if not edges:
        st.info("ê´€ê³„(ì—ì§€)ê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    G = nx.Graph()
    for rel in edges:
        s = rel.get("source"); t = rel.get("target"); lbl = rel.get("relation", "")
        if s and t:
            G.add_edge(s, t, label=lbl)

    pos = nx.spring_layout(G, dim=3, seed=42)
    x_nodes = [pos[n][0] for n in G.nodes()]
    y_nodes = [pos[n][1] for n in G.nodes()]
    z_nodes = [pos[n][2] for n in G.nodes()]

    edge_x, edge_y, edge_z = [], [], []
    for e in G.edges():
        x0, y0, z0 = pos[e[0]]
        x1, y1, z1 = pos[e[1]]
        edge_x += [x0, x1, None]
        edge_y += [y0, y1, None]
        edge_z += [z0, z1, None]

    fig = go.Figure()
    fig.add_trace(go.Scatter3d(
        x=x_nodes, y=y_nodes, z=z_nodes,
        mode="markers+text",
        marker=dict(size=8),
        text=list(G.nodes()),
        textposition="top center"
    ))
    fig.add_trace(go.Scatter3d(
        x=edge_x, y=edge_y, z=edge_z,
        mode="lines",
        line=dict(width=2)
    ))
    fig.update_layout(title=title, showlegend=False, height=520,
                      font=dict(family=PLOTLY_FONT))
    st.plotly_chart(fig, use_container_width=True)

# =========================
# ë¶„ì„(ìŠ¤íŠ¸ë¦¬ë°) + ë Œë”
# =========================
def analyze_and_render_streaming(name: str, all_text: str, model):
    # 1) ì˜¨í†¨ë¡œì§€ ìŠ¤íŠ¸ë¦¬ë°
    st.markdown("#### ğŸ“¡ ì˜¨í†¨ë¡œì§€ ìƒì„± ìŠ¤íŠ¸ë¦¼")
    raw_ontology_text = stream_generate_text(model, request_ontology_prompt(all_text), lang_hint="json")
    ontology = parse_json_safely(raw_ontology_text)
    if not ontology:
        st.error("ì˜¨í†¨ë¡œì§€ JSON íŒŒì‹± ì‹¤íŒ¨ (ìŠ¤íŠ¸ë¦¼ ì›ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”).")
        return

    st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ (JSON)")
    st.code(json.dumps(ontology, ensure_ascii=False, indent=2), language="json")

    if ontology.get("relationships"):
        st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ (ê´€ê³„ í…Œì´ë¸”)")
        st.dataframe(pd.DataFrame(ontology["relationships"]), use_container_width=True)

        st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ 3D ê·¸ë˜í”„")
        plot_ontology_3d(ontology, f"{name} ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„")

    # 2) ìš”ì•½ ìŠ¤íŠ¸ë¦¬ë°
    st.markdown("#### ğŸ“¡ ê³¼ê±° ìš”ì•½ ìƒì„± ìŠ¤íŠ¸ë¦¼")
    raw_summary_text = stream_generate_text(model, request_summary_prompt(all_text), lang_hint="markdown")
    summary = raw_summary_text.strip()

    st.markdown("#### ğŸ“Œ ê³¼ê±° ìš”ì•½")
    st.write(summary)

    # 3) ìµœì¢… í”„ë¡¬í”„íŠ¸
    final_prompt = build_final_prompt(name, ontology, summary)
    st.markdown("#### ğŸ§¾ ìµœì¢… í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´Â·ê³ ê°ì—­í• )")
    st.code(final_prompt, language="markdown")

    # ë‹¤ìš´ë¡œë“œ
    st.download_button(
        label="â¬‡ï¸ í”„ë¡¬í”„íŠ¸ ì €ì¥ (.txt)",
        data=final_prompt.encode("utf-8"),
        file_name=f"{name}_final_prompt.txt",
        mime="text/plain",
        key=f"dl_{name}"
    )

    # ìºì‹œ ì €ì¥
    st.session_state.analysis[name] = {
        "ontology": ontology,
        "summary": summary,
        "final_prompt": final_prompt,
    }

# =========================
# ë³¸ë¬¸ UI
# =========================
st.markdown("### 1) Notion DB ë¶ˆëŸ¬ì˜¤ê¸°")
if st.button("ğŸ“¥ Notion DB ë¶ˆëŸ¬ì˜¤ê¸°", type="primary"):
    if not (NOTION_TOKEN and NOTION_DB_ID):
        st.error("Notion Tokenê³¼ ì›ë³¸ DB IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        try:
            notion = Client(auth=NOTION_TOKEN)
            rows = fetch_notion_rows(notion, NOTION_DB_ID)
            st.session_state.df_rows = pd.DataFrame(rows)
        except Exception as e:
            st.error(f"Notion ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

df = st.session_state.df_rows
if df is not None and not df.empty:
    st.dataframe(df, use_container_width=True, height=360)
    st.markdown("---")

    st.markdown("### 2) ì´ë¦„ ê¸°ì¤€ ê·¸ë£¹í™” (ìœ ì €ë³„ ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°)")
    grouped = df.groupby("ì´ë¦„")["í…ìŠ¤íŠ¸"].apply(lambda s: "\n\n".join([x for x in s if x])).reset_index()
    st.session_state.grouped = grouped

    prev = grouped.copy()
    prev["í…ìŠ¤íŠ¸ ìƒ˜í”Œ"] = prev["í…ìŠ¤íŠ¸"].str.slice(0, 80) + "..."
    st.dataframe(prev[["ì´ë¦„", "í…ìŠ¤íŠ¸ ìƒ˜í”Œ"]], use_container_width=True, height=280)
    st.markdown("---")

    st.markdown("### 3) ìœ ì €ë³„ ë¶„ì„ (ë²„íŠ¼ ëˆ„ë¥´ë©´ ì‹¤í–‰)")
    if GEMINI_API_KEY:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            model = genai.GenerativeModel(model_name=MODEL_NAME)
        except Exception as e:
            model = None
            st.error(f"Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    else:
        model = None

    for idx, r in grouped.iterrows():
        name = r["ì´ë¦„"]; all_text = r["í…ìŠ¤íŠ¸"]
        with st.expander(f"ğŸ‘¤ {name}", expanded=False):
            st.caption("â€» ë²„íŠ¼ì„ ëˆ„ë¥´ê¸° ì „ê¹Œì§€ëŠ” ë¶„ì„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            if st.button("ğŸ§ª í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ", key=f"extract_{idx}"):
                if not model:
                    st.error("Gemini API Keyë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ëª¨ë¸ì„ ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”.")
                elif name in st.session_state.analysis:
                    st.info("ì´ë¯¸ ë¶„ì„ëœ ê²°ê³¼ê°€ ìˆì–´ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¬ë¶„ì„í•˜ë ¤ë©´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨.")
                    cached = st.session_state.analysis[name]
                    st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ (JSON)")
                    st.code(json.dumps(cached["ontology"], ensure_ascii=False, indent=2), language="json")

                    if cached["ontology"].get("relationships"):
                        st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ (ê´€ê³„ í…Œì´ë¸”)")
                        st.dataframe(pd.DataFrame(cached["ontology"]["relationships"]), use_container_width=True)
                        st.markdown("#### ğŸ“Œ ì˜¨í†¨ë¡œì§€ 3D ê·¸ë˜í”„")
                        plot_ontology_3d(cached["ontology"], f"{name} ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„")

                    st.markdown("#### ğŸ“Œ ê³¼ê±° ìš”ì•½")
                    st.write(cached["summary"])

                    st.markdown("#### ğŸ§¾ ìµœì¢… í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´Â·ê³ ê°ì—­í• )")
                    st.code(cached["final_prompt"], language="markdown")
                    st.download_button(
                        label="â¬‡ï¸ í”„ë¡¬í”„íŠ¸ ì €ì¥ (.txt)",
                        data=cached["final_prompt"].encode("utf-8"),
                        file_name=f"{name}_final_prompt.txt",
                        mime="text/plain",
                        key=f"dl_cached_{name}"
                    )
                else:
                    analyze_and_render_streaming(name, all_text, model)
else:
    st.info("ì‚¬ì´ë“œë°”ì— í‚¤ë¥¼ ì…ë ¥í•˜ê³ , ë¨¼ì € **ğŸ“¥ Notion DB ë¶ˆëŸ¬ì˜¤ê¸°**ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
